{
  "name": "Ollama",
  "id": "ollama",
  "type": "openai",
  "api_key": "",
  "api_endpoint": "http://localhost:11434/v1",
  "default_large_model_id": "llama3.1:70b",
  "default_small_model_id": "llama3.1:8b",
  "default_headers": null,
  "models": [
    {
      "id": "llama3.1:70b",
      "name": "Llama 3.1 70B",
      "cost_per_1m_in": 0.0,
      "cost_per_1m_out": 0.0,
      "cost_per_1m_in_cached": 0.0,
      "cost_per_1m_out_cached": 0.0,
      "context_window": 128000,
      "default_max_tokens": 8192,
      "can_reason": false,
      "has_reasoning_efforts": false,
      "default_reasoning_effort": null,
      "supports_attachments": false
    },
    {
      "id": "llama3.1:8b",
      "name": "Llama 3.1 8B",
      "cost_per_1m_in": 0.0,
      "cost_per_1m_out": 0.0,
      "cost_per_1m_in_cached": 0.0,
      "cost_per_1m_out_cached": 0.0,
      "context_window": 128000,
      "default_max_tokens": 8192,
      "can_reason": false,
      "has_reasoning_efforts": false,
      "default_reasoning_effort": null,
      "supports_attachments": false
    },
    {
      "id": "llama3.2:3b",
      "name": "Llama 3.2 3B",
      "cost_per_1m_in": 0.0,
      "cost_per_1m_out": 0.0,
      "cost_per_1m_in_cached": 0.0,
      "cost_per_1m_out_cached": 0.0,
      "context_window": 128000,
      "default_max_tokens": 8192,
      "can_reason": false,
      "has_reasoning_efforts": false,
      "default_reasoning_effort": null,
      "supports_attachments": false
    },
    {
      "id": "mistral:7b",
      "name": "Mistral 7B",
      "cost_per_1m_in": 0.0,
      "cost_per_1m_out": 0.0,
      "cost_per_1m_in_cached": 0.0,
      "cost_per_1m_out_cached": 0.0,
      "context_window": 32768,
      "default_max_tokens": 4096,
      "can_reason": false,
      "has_reasoning_efforts": false,
      "default_reasoning_effort": null,
      "supports_attachments": false
    },
    {
      "id": "codellama:13b",
      "name": "CodeLlama 13B",
      "cost_per_1m_in": 0.0,
      "cost_per_1m_out": 0.0,
      "cost_per_1m_in_cached": 0.0,
      "cost_per_1m_out_cached": 0.0,
      "context_window": 16384,
      "default_max_tokens": 4096,
      "can_reason": false,
      "has_reasoning_efforts": false,
      "default_reasoning_effort": null,
      "supports_attachments": false
    },
    {
      "id": "phi3:medium",
      "name": "Phi-3 Medium",
      "cost_per_1m_in": 0.0,
      "cost_per_1m_out": 0.0,
      "cost_per_1m_in_cached": 0.0,
      "cost_per_1m_out_cached": 0.0,
      "context_window": 128000,
      "default_max_tokens": 4096,
      "can_reason": false,
      "has_reasoning_efforts": false,
      "default_reasoning_effort": null,
      "supports_attachments": false
    },
    {
      "id": "qwen2.5:7b",
      "name": "Qwen 2.5 7B",
      "cost_per_1m_in": 0.0,
      "cost_per_1m_out": 0.0,
      "cost_per_1m_in_cached": 0.0,
      "cost_per_1m_out_cached": 0.0,
      "context_window": 32768,
      "default_max_tokens": 4096,
      "can_reason": false,
      "has_reasoning_efforts": false,
      "default_reasoning_effort": null,
      "supports_attachments": false
    },
    {
      "id": "gemma2:9b",
      "name": "Gemma 2 9B",
      "cost_per_1m_in": 0.0,
      "cost_per_1m_out": 0.0,
      "cost_per_1m_in_cached": 0.0,
      "cost_per_1m_out_cached": 0.0,
      "context_window": 8192,
      "default_max_tokens": 4096,
      "can_reason": false,
      "has_reasoning_efforts": false,
      "default_reasoning_effort": null,
      "supports_attachments": false
    }
  ]
}
